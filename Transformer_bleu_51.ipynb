{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SoS9iWQKYCTl"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def get_enc():\n",
        "  return \"UTF-8\"\n",
        "locale.getpreferredencoding = get_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0LBwxyZBVJqr"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lllx_xn5XI9x"
      },
      "outputs": [],
      "source": [
        "!pip install torchdata -q\n",
        "!pip install portalocker -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC6QLLLVJqt"
      },
      "source": [
        "\n",
        "# Language Translation with ``nn.Transformer`` and torchtext\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer.\n",
        "    - Use torchtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)_ dataset to train a German to English translation model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRoCvByUVJqv"
      },
      "source": [
        "## Data Sourcing and Processing\n",
        "\n",
        "[torchtext library](https://pytorch.org/text/stable/)_ has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to use torchtext's inbuilt datasets,\n",
        "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k)_\n",
        "that yields a pair of source-target raw sentences.\n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SfiTpOB7VJqw"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import random_split\n",
        "from typing import Iterable, List\n",
        "import re\n",
        "\n",
        "SRC_LANGUAGE = 'crh'\n",
        "TGT_LANGUAGE = 'rus'\n",
        "\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "utdJQBqH8Kt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e2f4b-0649-44e6-f74d-fce18e23da45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23281\n",
            "31422\n"
          ]
        }
      ],
      "source": [
        "def read_dataset(path='data.txt'):\n",
        "  dataset = []\n",
        "  for line in open(path):\n",
        "    pair = line.split('\\t')\n",
        "    if len(pair) < 2:\n",
        "      continue\n",
        "    src, trg = line.split('\\t')\n",
        "    trg = re.sub(r'\\s(ст\\s|ч\\s)+', ' ', trg)\n",
        "    trg = trg.strip('\\n').strip('')\n",
        "    data = {}\n",
        "    data[TGT_LANGUAGE] = trg\n",
        "    data[SRC_LANGUAGE] = src\n",
        "    dataset.append(data)\n",
        "  return dataset\n",
        "\n",
        "def replicate_phrases(dataset = []):\n",
        "  ext = []\n",
        "  for pair in dataset:\n",
        "    trg = pair['rus']\n",
        "    words = len(trg.split(' '))\n",
        "    if words > 0 and words < 6:\n",
        "      ext.append(pair)\n",
        "  dataset.extend(ext)\n",
        "  return dataset\n",
        "\n",
        "dataset = read_dataset('./data_clean_final.tsv')\n",
        "proportions = [.8, .15, .05] \n",
        "lengths = [int(p * len(dataset)) for p in proportions]\n",
        "lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
        "train_data, valid_data, test_data = random_split(dataset, lengths)\n",
        "print(len(train_data))\n",
        "train_data = replicate_phrases(list(train_data))\n",
        "print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle \n",
        "\n",
        "shuffle(train_data)"
      ],
      "metadata": {
        "id": "2zYiE43KuvUR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o25H_hru_SQ",
        "outputId": "292bd735-3df9-4306-8467-e7582c8a3c70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rus': 'контроль за выполнением настоящего постановления возложить на комитет государ ственного совета республики крым по инвестиционной и налоговой политике и комитет государственного совета республики крым по информационной полити ке информационным технологиям и связи',\n",
              " 'crh': 'мезкюр къарар ерине кетирильмеси узеринде незарет къырым джумхуриети девлет шурасынынъ ятырым ве берги сиясети боюнджа комитети ве къырым джум хуриети девлет шурасынынъ малюмат сиясети малюмат технологиялары ве алякъа боюнджа комитетине авале этильсин'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ],
      "metadata": {
        "id": "QsBH2jzGxzPV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
        "tokenizer_src = Tokenizer(BPE(unk_token = '<unk>'))\n",
        "tokenizer_trg = Tokenizer(BPE(unk_token = '<unk>'))\n",
        "\n",
        "# Then we enable lower-casing and unicode-normalization\n",
        "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
        "# executed in order.\n",
        "tokenizer_src.normalizer = Sequence([\n",
        "  NFKC(),\n",
        "  Lowercase()\n",
        "])\n",
        "tokenizer_trg.normalizer = Sequence([\n",
        "  NFKC(),\n",
        "  Lowercase()\n",
        "])\n",
        "tokenizer_src.add_special_tokens(['<unk>', '<sos>', '<eos>', '<pad>'])\n",
        "tokenizer_trg.add_special_tokens(['<unk>', '<sos>', '<eos>', '<pad>'])\n",
        "\n",
        "tokenizer_src.enable_padding(pad_token = '<pad>')\n",
        "tokenizer_trg.enable_padding(pad_token = '<pad>')\n",
        "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
        "tokenizer_src.pre_tokenizer = ByteLevel()\n",
        "tokenizer_trg.pre_tokenizer = ByteLevel()\n",
        "\n",
        "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
        "tokenizer_src.decoder = ByteLevelDecoder()\n",
        "tokenizer_trg.decoder = ByteLevelDecoder()\n",
        "\n",
        "tokenizer_src.decoder = ByteLevelDecoder()\n",
        "tokenizer_trg.decoder = ByteLevelDecoder()\n",
        "\n",
        "tokenizer_src.post_processor = TemplateProcessing(\n",
        "    single=\"<sos> $A <eos>\",\n",
        "    #pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"<sos>\", 1), (\"<eos>\", 2)],\n",
        ")\n",
        "\n",
        "tokenizer_trg.post_processor = TemplateProcessing(\n",
        "    single=\"<sos> $A <eos>\",\n",
        "    #pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"<sos>\", 1), (\"<eos>\", 2)],\n",
        ")"
      ],
      "metadata": {
        "id": "nM_0NIKv0WuE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(tokenizer, data, lang, save_folder):  \n",
        "  def yield_tokens(data, lang):\n",
        "    for sample in data:\n",
        "      yield sample[lang]\n",
        "  # We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
        "  trainer = BpeTrainer(vocab_size = 15000, show_progress=True, \n",
        "                       specials=['<unk>', '<sos>', '<eos>', '<pad>'],\n",
        "                       initial_alphabet=ByteLevel.alphabet())\n",
        "  tokenizer.train_from_iterator(yield_tokens(data, lang), trainer=trainer)\n",
        "\n",
        "  print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))\n",
        "  tokenizer.model.save(save_folder)"
      ],
      "metadata": {
        "id": "Hk9t0lSS0gLE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir trg src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB72KVTdqp-5",
        "outputId": "fe8d0cce-4779-4aaf-bd5c-b2e2219e434f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘trg’: File exists\n",
            "mkdir: cannot create directory ‘src’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_vocab(tokenizer_trg, train_data, SRC_LANGUAGE, './trg')\n",
        "create_vocab(tokenizer_src, train_data, TGT_LANGUAGE, './src')\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = tokenizer_src\n",
        "token_transform[TGT_LANGUAGE] = tokenizer_trg\n",
        "\n",
        "vocab_transform[SRC_LANGUAGE] = token_transform[SRC_LANGUAGE].get_vocab()\n",
        "vocab_transform[TGT_LANGUAGE] = token_transform[TGT_LANGUAGE].get_vocab()"
      ],
      "metadata": {
        "id": "nx-ee-Ok0hY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159504b8-34d8-45fd-fb15-9f9287246649"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained vocab size: 15004\n",
            "Trained vocab size: 15004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_src.model = BPE('./src/vocab.json', './src/merges.txt')\n",
        "tokenizer_trg.model = BPE('./trg/vocab.json', './trg/merges.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEBRcvOP-mJ5",
        "outputId": "8e83fbdb-6ceb-4929-840d-286890e051f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-1a277977d10b>:1: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
            "  tokenizer_src.model = BPE('./src/vocab.json', './src/merges.txt')\n",
            "<ipython-input-17-1a277977d10b>:2: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
            "  tokenizer_trg.model = BPE('./trg/vocab.json', './trg/merges.txt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_src.encode(train_data[4]['crh']).tokens)\n",
        "print(tokenizer_trg.encode(train_data[4]['rus']).tokens)\n",
        "print(tokenizer_src.encode(train_data[4]['crh']).ids)\n",
        "print(tokenizer_src.decode(tokenizer_src.encode(train_data[4]['rus']).ids))\n",
        "print(tokenizer_trg.decode(tokenizer_trg.encode(train_data[4]['crh']).ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BneuMz1R_Wxm",
        "outputId": "9fc0dc2a-5b10-4c76-95c0-59247ece82d9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'ĠÑĤÐµ', 'ÑĦ', 'ÑģÐ¸', 'ÑĢ', 'Ð»ÐµÑĢ', 'Ð´Ðµ', 'ĠÑĤÐ°ÑĢ', 'ÑĤÑĭ', 'ÑĪ', 'Ð¼Ð°', '<eos>']\n",
            "['<sos>', 'ĠÐ½Ðµ', 'ĠÑģÐ¿', 'Ð¾ÑĢ', 'ÑĮ', 'ĠÐ²', 'ĠÐºÐ¾Ð¼', 'Ð¼ÐµÐ½', 'ÑĤÐ°ÑĢ', 'Ð¸Ñı', 'Ñħ', '<eos>']\n",
            "[1, 445, 431, 676, 264, 9808, 484, 12608, 638, 380, 486, 2]\n",
            " не спорь в комментариях\n",
            " тефсирлерде тартышма\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JVzPSHycVJqy"
      },
      "outputs": [],
      "source": [
        "# token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='ru_core_news_sm')\n",
        "# token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# # helper function to yield list of tokens\n",
        "# def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "#     for data_sample in data_iter:\n",
        "#         yield token_transform[language](data_sample[language])\n",
        "\n",
        "# # Define special symbols and indices\n",
        "UNK_IDX = 0\n",
        "SOS_IDX = 1 \n",
        "EOS_IDX = 2\n",
        "PAD_IDX = tokenizer_src.padding['pad_id']\n",
        "# # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "#     # Training data Iterator\n",
        "#     train_iter = iter(dataset) # todo: change\n",
        "#     # Create torchtext's Vocab object\n",
        "#     vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_data, ln),\n",
        "#                                                     min_freq=1,\n",
        "#                                                     specials=special_symbols,\n",
        "#                                                     special_first=True)\n",
        "\n",
        "# # Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# # If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "# for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "#   vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1GrqZiM5ZfRU"
      },
      "outputs": [],
      "source": [
        "# print(f\"Vocab has {len(vocab_transform['en'])} english words and {len(vocab_transform['ru'])} russian words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkdRVf0xVJqz"
      },
      "source": [
        "## Seq2Seq Network using Transformer\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [“Attention is all you\n",
        "need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\n",
        "paper for solving machine translation tasks.\n",
        "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "encodings to provide position information of input tokens to the model. The second part is the\n",
        "actual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model.\n",
        "Finally, the output of the Transformer model is passed through linear layer\n",
        "that gives unnormalized probabilities for each token in the target language.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jge9721hVJq0"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70udq2SGVJq1"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the model from looking into\n",
        "the future words when making predictions. We will also need masks to hide\n",
        "source and target padding tokens. Below, let's define a function that will take care of both.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1EGtM7JFVJq2"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqoufKYLVJq3"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also\n",
        "define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1DuQnml5VJq4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 96\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vKxublKVJq5"
      },
      "source": [
        "## Collation\n",
        "\n",
        "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings.\n",
        "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network\n",
        "defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\n",
        "can be fed directly into our model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "x4XxgW6EVJq6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([SOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "def my_transform(tokenizer, sample):\n",
        "  return tensor_transform(tokenizer.encode(sample).ids)\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for sample in batch:\n",
        "        src_sample, tgt_sample = sample[SRC_LANGUAGE], sample[TGT_LANGUAGE]\n",
        "        src_batch.append(my_transform(token_transform[SRC_LANGUAGE], src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(my_transform(token_transform[TGT_LANGUAGE], tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm4I4ybEVJq6"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each\n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rGWIBh96VJq6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    \n",
        "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_svND8fDHGZ",
        "outputId": "14c01030-641f-43ad-cc51-07ef9e18232b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIUW-gjuVJq7"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQ_3UacVJq8",
        "outputId": "01133245-79f5-475d-f3f8-72cd972f6d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 4.769, Val loss: 2.978, Epoch time = 97.574s\n",
            "Epoch: 2, Train loss: 2.712, Val loss: 2.293, Epoch time = 99.285s\n",
            "Epoch: 3, Train loss: 2.226, Val loss: 1.954, Epoch time = 99.458s\n",
            "Epoch: 4, Train loss: 1.948, Val loss: 1.745, Epoch time = 98.997s\n",
            "Epoch: 5, Train loss: 1.757, Val loss: 1.607, Epoch time = 97.580s\n",
            "Epoch: 6, Train loss: 1.615, Val loss: 1.503, Epoch time = 99.393s\n",
            "Epoch: 7, Train loss: 1.500, Val loss: 1.420, Epoch time = 99.070s\n",
            "Epoch: 8, Train loss: 1.403, Val loss: 1.356, Epoch time = 98.769s\n",
            "Epoch: 9, Train loss: 1.321, Val loss: 1.306, Epoch time = 99.052s\n",
            "Epoch: 10, Train loss: 1.248, Val loss: 1.266, Epoch time = 97.806s\n",
            "Epoch: 11, Train loss: 1.183, Val loss: 1.229, Epoch time = 98.956s\n",
            "Epoch: 12, Train loss: 1.125, Val loss: 1.197, Epoch time = 99.356s\n",
            "Epoch: 13, Train loss: 1.071, Val loss: 1.173, Epoch time = 102.528s\n",
            "Epoch: 14, Train loss: 1.022, Val loss: 1.147, Epoch time = 103.725s\n",
            "Epoch: 15, Train loss: 0.978, Val loss: 1.133, Epoch time = 103.637s\n",
            "Epoch: 16, Train loss: 0.936, Val loss: 1.114, Epoch time = 103.960s\n",
            "Epoch: 17, Train loss: 0.896, Val loss: 1.104, Epoch time = 104.311s\n",
            "Epoch: 18, Train loss: 0.859, Val loss: 1.093, Epoch time = 104.007s\n",
            "Epoch: 19, Train loss: 0.824, Val loss: 1.086, Epoch time = 104.310s\n",
            "Epoch: 20, Train loss: 0.792, Val loss: 1.081, Epoch time = 104.663s\n",
            "Epoch: 21, Train loss: 0.761, Val loss: 1.076, Epoch time = 103.333s\n",
            "Epoch: 22, Train loss: 0.732, Val loss: 1.069, Epoch time = 104.249s\n",
            "Epoch: 23, Train loss: 0.706, Val loss: 1.071, Epoch time = 103.903s\n",
            "Epoch: 24, Train loss: 0.679, Val loss: 1.068, Epoch time = 103.217s\n",
            "Epoch: 25, Train loss: 0.653, Val loss: 1.069, Epoch time = 103.838s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform[SRC_LANGUAGE].encode(\"я ем тост с маслом\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H8O_RWalPe3",
        "outputId": "5007028e-9261-4b2c-88cf-3c2f04d52d32"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 539, 7349, 1093, 269, 283, 3352, 1812, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask).to(DEVICE)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        # memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    # model.eval()\n",
        "    src = torch.tensor(token_transform[SRC_LANGUAGE].encode(src_sentence).ids).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model, src, src_mask, max_len=num_tokens + 5, start_symbol=SOS_IDX).flatten()\n",
        "    return \"\".join(token_transform[TGT_LANGUAGE].decode(list(tgt_tokens))).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "411yfQP5HmcG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGxC2LahacGF",
        "outputId": "4abedda3-b8ec-4180-8ddd-0a0c8f1fcb4c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rus': 'нам нужен номер на двоих', 'crh': 'визге эки кишилик номер керек'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFgd0HbwmyHs",
        "outputId": "04a1d34d-9802-46e2-c468-a8d7a062a806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< къырым джумхуриети девлет шур асынынъ къар ары айванлар алеми акъкъында къырым джумхуриети къанунынынъ маддесине денъишмелер кирсетильмеси акъкъында къырым джумхуриетининъ къануны акъкъында къырым джумхуриети анаясасынынъ маддесиндеки пунктына маддесиндеки къысмына къырым джумхуриети девлет шурасы регламентининъ мад десиндеки къысмына маддесине мувафыкъ къырым джумхуриетининъ девлет шурасы къарар бере\n",
            "=== постановление государственного совета республики крым о законе республики крым о внесении изменений в статью закона республики крым о животном мире в соответствии с пунктом стат ьи частью статьи конституции республики крым частью статьи статьей регламента государственного совета республики крым государственный совет республики крым постановляет\n",
            ">>>  постановление государственного совета республики крым о законе республики крым о внесении изменений в статью закона республики крым о животном мире в соответствии с пунктом статьи частью статьи конституции республики крым частью статьи статьей регламента государственного совета республики крым государственный совет республики крым постановляет\n",
            "********************\n",
            "<<< мен шимди чалышмайым\n",
            "=== я сейчас не работаю\n",
            ">>>  я работаю я не вставил\n",
            "********************\n",
            "<<< къырым джумхуриети ички сиясет информация ве багъ везирлигине тевсие этильсин ички сиясет информация ве багъ саасында макъсадлы программаларнынъ къырым джумхуриетининъ девлет программалары ве русие федерациясынынъ девлет программа ларынен бельгиленген тедбирлернинъ вакътында кечирильмесини темин этмек къырым джумхуриети ве русие федерациясы къанунджылыгъынынъ амельдеки къаи делерине мутенасип къырым джумхуриетинде беледие тешкиллернинъ ерли мустакъиль\n",
            "=== рекомендовать министерству внутренней политики информации и связи республики крым обеспечить своевременное выполнение мероприятий предусмотренных государственны ми программами российской федерации и государственными программами республики крым целевыми программами в сфере внутренней политики информации и связи организовать совместно с органами местного самоуправления муниципальных образова ний в республике крым работу по упорядочению размещения наружной\n",
            ">>>  рекомендовать министерству финансов республики крым обеспечить финансирование мероприятий связанных с финансов республики крым а также программы и обеспечения эффективности внутренней политики информации о вопросах связи с противоминной деятельности государственной думы федерального собра российской федерации принимать мероприятий связи с противоминной службы и обеспечением информации о выполнительней деятельности в\n",
            "********************\n",
            "<<< бу керчек бир киднаппингдир огренгенимиз тюм вакъиалар акъкъында аджеле тарзда халкъара муэссиселерни хабердар этемиз ве балаларны къайтармагъа уныджеф ве дигер тешкилятларнынъ балаларымызнынъ украинагъа къайтарылмасыны эльге алмакъ ичюн семерели адынлар атмасыны талап этемиз деп вице баш назири хабердар этти\n",
            "=== это настоящий киднеппинг обо всех известных нам случаях мы сразу сообщаем международные структуры и пытаемся вернуть детей от юнисеф и других организаций требуем действенных шагов по решению проблемы возвращения наших детей в украину сообщила\n",
            ">>>  такие действия внимание что речь идет она должны быть международными организациями в украине сообщила международному сообществу поддержке многие людей которые нуждаются вместе с тем кто вывезли речь идет все вытащают детей остались в безопасные регионы украины ирина верещук\n",
            "********************\n",
            "<<< мад\n",
            "=== ст \n",
            ">>>  ст \n",
            "********************\n",
            "<<< одеколон\n",
            "=== одеколон\n",
            ">>>  ручатель\n",
            "********************\n",
            "<<< куль тюс\n",
            "=== пепельный\n",
            ">>>  смеяться\n",
            "********************\n",
            "<<< сонъки рейс\n",
            "=== последний рейс\n",
            ">>>  голос человек\n",
            "********************\n",
            "<<< ташылып олгъан\n",
            "=== переносной\n",
            ">>>  незнакомый\n",
            "********************\n",
            "<<< чёплеме от\n",
            "=== чемерица\n",
            ">>>  словошина\n",
            "********************\n",
            "<<< догъмакъ\n",
            "=== родиться\n",
            ">>>  верно\n",
            "********************\n",
            "<<< мен москвадан кельдим\n",
            "=== я приехал из москвы\n",
            ">>>  я роственное прише сутки\n",
            "********************\n",
            "<<< къырым мухтар джумхуриети юкъары радасынынъ сенеси сентябрь къабул олунгъан санлы къарарынен тасдикълангъан ерли месулиетли ер устю сув объектлерини файдаланувгъа иджаре шартларында такъдим этюв тертибине мад мухтар джумхуриетининъ актлар топламы мад ашагъыдаки денъ ишмелер кирсетильсин пунктынынъ экинджи сатыр башы ашагъыдаки тарирде тарз этильсин ерли месулиетли сув объектлери сырасына кирген ер устю\n",
            "=== внести в порядок предоставления в пользование поверхностных водных объектов местного значения на условиях аренды утвержденный постановлением верховной рады автономной республики крым от сентября года сборник нормативноправовых актов автономной республики крым следующие изменения абзац второй пункта изложить в следующей редакции поверхностные водные объекты которые относятся к водным объектам местного значения\n",
            ">>>  внести в постановление верховной рады автономной республики крым от сентября года о порядке предоставления в пользование повышенной эффективности водных объектах местного значения в автономной республике крым сборник актов автономной республики крым следующие изменения в пункте изложить в следующей редакции водных объектах объектах объектах объектах местного значения в автономной республике крым на\n",
            "********************\n",
            "<<< мад\n",
            "=== ст \n",
            ">>>  ст \n",
            "********************\n",
            "<<< къырым джумхуриети девлет шур асынынъ къар ары къырым джумхуриетининъ къанун лейихасы акъкъында къырым джумхуриети къырым джумхуриетининъ девлет гр ажданлыкъ хызметининъ вазифелер реестри акъкъында къанунына денъишмелер кирсетильмеси акъкъында къырым джумхуриети анаясасынынъ маддесининъ пунктына маддесине мувафыкъ къырым джумхуриетининъ башы вазифесини вакъытынджа эда эткен аксеновнен кирсетильген къырым джумхуриети къырым джумхуриети нинъ девлет гражданлыкъ\n",
            "=== постановление государственного совета республики крым о проекте закона республики крым о внесении изменений в закон республики крым о реестре должностей государственной гр ажданской службы республики крым в соответствии с пунктом статьи статьей конституции республики крым рас смотрев проект закона республики крым о внесении изменений в закон республи ки крым\n",
            ">>>  постановление государственного совета республики крым о проекте закона республики крым о внесении изменений в закон республики крым о реестре должностей государственной гр ажданской службы республики крым в соответствии с пунктом статьи частью статьи конституции республики крым рассмотрев проект закона республики крым о реестре должностей государственной гр ажданской службы республики крым рег\n",
            "********************\n",
            "<<< къырым мухтар джумхуриетининъ мулькю теркибине багъчасарай районы почтовое шчкъ территориаль джемаатчылыгъынынъ коммуналь мулькиетинден км узунлыгъында басымлы сув агъыны симферополь районы трудовое коюнинъ территориаль джемаатчылыгъынынъ коммуналь мулькиетинден симферополь районы трудовое кой шурасынынъ яшалгъан массивде ерлешкен сув теминлев агъларыны къабул этмеге разылыкъ берильсин\n",
            "=== дать согласие на прием в состав имущества принадлежащего автономной республике крым из коммунальной собственности территориальной громады пгт почтовое бахчисарайского района напорного водовода протяженностью км из коммунальной собственности территориальной громады трудового симферопольского района сетей водоснабжения расположенных в жилом массиве трудовского сельского совета симферопольского района\n",
            ">>>  передать из состава имущества принадлежащего автономной республике крым в коммунальную собственность территориальной громады сел северского сельского совета объект незавершенного строительства объект незавершенного строительства реконструкция жилого дома в районе автономной республике крым в коммунальную собственность территориальной громады сел северной громады сел лескова окпо\n",
            "********************\n",
            "<<< бугунь акъшам мешгъульсинъизми\n",
            "=== вы не заняты сегодня вечером\n",
            ">>>  сегодня вечером сегодня вечером\n",
            "********************\n",
            "<<< кийик зюмбюль\n",
            "=== пролеска\n",
            ">>>  безрабрый\n",
            "********************\n",
            "<<< башкъа маркалы душ гели бармы\n",
            "=== у вас есть гель для душа другой марки\n",
            ">>>  есть мне сказаться у меня есть орехов\n",
            "********************\n",
            "<<< бу сене эр ай боюнджа реинтеграция назирлиги кремл сиясий мабюслерине левко лукьяненко адына девлет стипендиясыны одей эди бутюн бу вакъыт девамында киши умумен млн гривналыкъ стипендия алды\n",
            "=== в течение года минреинтеграции ежемесячно выплачивало государственные стипендии имени левко лукьяненко политзаключенным кремля за это время человек получили стипендии на общую сумму млн гривен\n",
            ">>>  в году минреинтеграции поступает основные стипендии имени левко лукьяненко за ноябрь такие выплаты получили украинских пленных украинцев\n",
            "********************\n",
            "<<< вице баш назир ирина верешчукнынъ эмрине коре азат этильгенлерни реинтеграция назирлигинде къаршылаштылар инсанлар эвлерине къайтмадан эвель яхшылашмалылар ве тиббий бакъынувдан кечмелилер\n",
            "=== по поручению ирины верещук уволенных встречали в минреинтеграции перед отправкой домой люди должны восстановиться и пройти медицинское обследование\n",
            ">>>  по результатам совещания минреинтеграции разрушеных по освобожденных заявлений на основе расстояние повреждения поврежденных мероприятий по разминированию\n",
            "********************\n",
            "<<< март айынынъ ичюн гуманитар коридор келишильди ирина верешчук\n",
            "=== марта согласовано гуманитарных верещук\n",
            ">>>  марта гуманитарных коридоров ирина верещук марта согласовано гуманитарных коридоров ирина верещук\n",
            "********************\n",
            "<<< вице баш назир украинанынъ вакътынджа ишгъаль этильген топракъларынынъ реинтеграция меселелери назири ирина верешчук апрель айынынъ ичюн планлангъан гуманитар коридроларны анълатты\n",
            "=== по вопросам реинтеграции временно окпованных территорий украины ирина верещук доложила о запланированной работе гуманитарных коридоров на апреля года\n",
            ">>>  по вопросам реинтеграции временно оккупированных территорий украины ирина верещук рассказала о рассказала о работе гуманитарных коридоров на апреля\n",
            "********************\n",
            "<<< кенъ манълай\n",
            "=== широкий лоб\n",
            ">>>  липая\n",
            "********************\n",
            "<<< айны заманда реназирлиги хатырлата ки девлет ичинде алынгъан кишилерни бедава мескенде ерлештиреджеклер малиевий гуманитар ве рухий ярдым этеджеклер ичтимаий озгъарув темин этеджеклер\n",
            "=== в свою очередь в минреинтеграции напоминают что внутренне перемещенных лиц разместят в бесплатных убежищах окажут финансовую гуманитарную и психологическую помощь социальное сопровождение\n",
            ">>>  вместе с тем время минреинтеграции обеспечивает финансирование гуманитарной помощи внутренних переселенцев бесплатных психологической помощи внутренних переселенцев\n",
            "********************\n",
            "<<< прывокзална демиръёл алякъа агъысы\n",
            "=== ул привокзальная узел связи железной дороги\n",
            ">>>  у вас есть более железнодорожных железнодорожных\n",
            "********************\n",
            "<<< бу сене январь айындан мыколайив виляетининъ ишгъальден азат этильген мескюн ерине пенсия кетирмек ве одемек ичюн техникий имкянлар янъыдан яратылды бундан гъайры декабрь айында виляеттеки снихуривка шеэринде упф хызмет меркезини янъыдан ачмагъа беджерди\n",
            "=== с января этого года восстановлена техническая возможность осуществления доставки и выплаты пенсий в деоккупированном населенном пункте николаевщины кроме того в декабре в области была возобновлена работа сервисного центра пфу в городе снигиревка\n",
            ">>>  в году этот день пенсионный фонд что освобожденных населенных пунктов харьковской области работа по восстановлению города киевской черниговской областей города сейчас основных областей города киевской черниговской областей в киевской области\n",
            "********************\n",
            "<<< муайен бир миллетнинъ геноцид джинаетини япкъан инсанларнынъ джеза алгъанлары ильк макеме джерьянларындан бири эди косьтеримли тарзда бутюн дюйна огюнде руслар да айны шекильде джезадан къачамазлар халкъара такъикъат оргналары ве бизим укъукъ къорчалайыджыларымыз бойле деген васийлернинъ адларыны энди огрендилер къанунсыз оларакъ улькеден кетирильген бизим оксюз балаларымызны васийлик этерек гуна япкъан инсанларгъа\n",
            "=== это был один из первых судебных процессов когда за геноцид определенной нации исполнители этого преступления получили реальное наказание показательно на весь мир так же не избежат ответственности и россияне международным следственным органам и нашим правоохранителям уже известны фамилии так называемых усыновителей и опекунов я в который раз напоминаю тем кто\n",
            ">>>  каждая россияне встречи россияне разные наших украину что россияне перемещенные лицаются на всех кто россияне перемещенные лицами наши освобожденные защитники наших родные должны быть что россиян всех усыновленные украинских детей лишенные российские оккупанты не только что россияной уже усыновленные что россияне не освободили депортации или депортированных украинцев наших детей\n",
            "********************\n",
            "<<< не так оппозиция блоку ндан чокъмандатлы сайлав больгеде сайлангъан къырым мухтар джумхуриети юкъары радасынынъ депутаты нестор иванович шуфричнинъ векялетлиги муддеттен эвель токътатылсын\n",
            "=== прекратить досрочно полномочия депутата верховной рады автономной республики крым избранного в многомандатном избирательном округе от оппозиционного блока не так шуфрича нестора ивановича\n",
            ">>>  прекратить досрочно полномочия депутата верховной рады автономной республики крым избранного в момандатном избирательном округе от крымского республиканского на срок полномочий верховной рады автономной республики крым в сроком на лет\n",
            "********************\n",
            "<<< реинтеграция назирлиги эсирликтен азат этильген украиналыларнынъ эсас ихтияджларыны къанаатлендиреджек кеченлерде укюмет бу тюр къанун берди весикъагъа муджиби реинтеграция назирлиги халкъара гуманитар укъукънынъ къаиделеринде эсапкъа алынгъан тедбирлерни кечирмеге вазифели тайинленди\n",
            "=== минреинтеграции будет обеспечивать основные потребности освобожденных из плена украинцев такое распоряжение приняло на днях правительство согласно документу министерство определено ответственным органом за осуществление мероприятий предусмотренных нормами международного гуманитарного права\n",
            ">>>  минреинтеграции перечислило млн украинцам которые освобождены из плена украинцам правительства минреинтеграции определенных правительственного определить перечень поврежденным лицам за пределами освобожденным из плена международным гуманитарным правительством правительством\n",
            "********************\n",
            "<<< мусафирчен эвинъизде раатлангъан куньлеримизни унутмамыз\n",
            "=== мы не забудем те дни когда отдыхали в вашем гостеприимном доме\n",
            ">>>  не ходите пожалуйста мы не заботились от очень мы\n",
            "********************\n",
            "<<< къырым джумхуриети девлет шур асынынъ къар ары маленко акъкъында къырым джумхуриети анаясасынынъ маддесиндеки пунктына мад десиндеки къысмынынъ пунктына сенеси майыс къабул олун гъан къырым джумхуриети девлет шурасы къырым джумхуриетининъ парламенти акъкъында санлы къырым джумхуриети къанунынынъ маддесиндеки къыс мына маддесиндеки къысмынынъ пунктына сенеси майыс къабул олунгъан къырым джумхуриетининъ девлет вазифелери акъкъында\n",
            "=== постановление государственного совета республики крым о маленко в соответствии с пунктом статьи пунктом части статьи конституции респуб лики крым частью статьи пунктом части статьи закона республики крым от мая года о государственном совете республики крым парламенте республики крым пунктом статьи закона республики крым от мая года о государ ственных должностях\n",
            ">>>  постановление государственного совета республики крым о запорожье о планах связи с собственности республики крым в соответствии с пунктом статьи конституции республики крым пунктом части статьи закона республики крым от мая года о государственном совете республики крым парламенте республики крым пунктом части статьи закона республики крым от мая года о государственном совете республики\n",
            "********************\n",
            "<<< къырым джумхуриети девлет шур асынынъ къар ары балаларгъа нефакъа акъкъында къырым джумхуриетининъ къанунына денъишмелер кирсетильмеси акъкъында къырым джумхуриетининъ къанун лейхасы акъкъында къырым джумхуриети анаясасынынъ маддесиндеки пунктына маддесиндеки къысмына къырым джумхуриети девлет шурасы низамнамесининъ iii бо люмдеки болюгине маддесининъ къысмына мувафыкъ къырым джумхури етининъ башы аксенов тарафындан кирсетильген балаларгъа нефакъа акъкъында\n",
            "=== постановление государственного совета республики крым о проекте закона республики крым о внесении изменений в закон республики крым о пособии на ребенка в соответствии с пунктом статьи частью статьи конституции республики крым главой раздела iii частью статьи регламента государственного совета республики крым рассмотрев проект закона республики крым о внесении изменений в\n",
            ">>>  постановление государственного совета республики крым о проекте закона республики крым о внесении изменений в закон республики крым о пособии на ребенка в соответствии с пунктом статьи частью статьи конституции республики крым рассмотрев проект закона республики крым о внесении изменений в закон республики крым о пособии на ребенка в соответствии с пунктом статьи частью статьи\n",
            "********************\n",
            "<<< сабыкъ мсдж керчь окъ заводы нынъ мулькю теркибинде ка дастр номералы мейданлыгъынен цсэп истисал хане бинасы кадастр номералы мейданлыгъынен сео\n",
            "=== имущество бывшего ооо керченский стрелочный завод в составе здание цеха цсэп площадью м кадастровый номер здание для печи сео\n",
            ">>>  согласовать безвозмездную передачу из площадью кадастровый номер площадью м кадастровый номер расположенный по адресу керчь ул севернее площадью га кадастровый номер\n",
            "********************\n",
            "<<< мал бакъыджы\n",
            "=== скотовод\n",
            ">>>  товарищик\n",
            "********************\n",
            "<<< вакътынджа къорума абнинъ азасы олгъан укленинъ сынъырыны кечкенде алыныр макъбуль улькенинъ девлет органлары кишини эльде этме вакъты пек тез ве мумкюн олгъандже баситлештирильген вакътынджа къорума алма акъкъы акъкъында хабердар этмели\n",
            "=== временную защиту получают при пересечении границы ес государственные органы соответствующей страны должны уведомить лицо о его праве на получение временной защиты процесс приобретения которой достаточно быстрый и максимально упрощенный\n",
            ">>>  время за внутренних переселенцев получения правительства в частности в пребывания за границу надлежащих правозащитников внутренних переселенцев получили работу во время пребывания внутренних переселенцев\n",
            "********************\n",
            "<<< къырым джумхуриети девлет шур асынынъ къар ары къырым джумхуриетинде балаларнынъ р аатлы гъыны ве сагъламлаштырмасыны тешкиль ве темин этильмеси акъкъында къырым джумхуриетининъ къанун лейхасы акъкъында къырым джумхуриети анаясасынынъ маддесиндеки пунктына мадде синдеки къысмына къырым джумхуриети девлет шурасы регламентининъ мад десиндеки къысмына мувафыкъ къырым джумхуриетининъ депутаты черняк тарафынд ан кир с\n",
            "=== постановление государственного совета республики крым о проекте закона республики крым об организации и обеспечении отдыха детей и их оздоровления в республике крым в соответствии с пунктом статьи частью статьи конституции республики крым частью статьи регламента государственного совета республики крым рассмотрев проект закона республики крым об организации и обеспечении отдыха детей\n",
            ">>>  постановление государственного совета республики крым о проекте закона республики крым об организации и обеспечении отдыха детей и их оздоровления в республике крым в соответствии с пунктом статьи частью статьи конституции республики крым частью статьи регламента государственного совета республики крым рассмотрев проект закона республики крым об организации и обеспечении отдыха детей\n",
            "********************\n",
            "<<< бундан гъайры вастасынен озь бильгилерини тешкермек я да файдалы бир видео сейир этмек мумкюн китапнынъ интераткив олмасы малюматны оюн шеклинде даа къолай огренмеге ярдымджы олур\n",
            "=== кроме того напоминаем что вышеупомянутые пособия являются интерактивными и если перейти по ссылкам в можно увидеть дополнительный материал или проверить свои знания\n",
            ">>>  кроме того позволяет своей свои просмотреть полезное видео интерактивность книжки и электронному проськажить свою работы можно узнать обезврежить свои\n",
            "********************\n",
            "<<< къырым джумхуриети девлет шур асынынъ къар ары къырым джумхуриети везир лер шур асынынъ реис му авини вазифесине кабанов тайинленмесини уйгъунлаштырма акъкъында къырым джумхуриети анаясасынынъ маддесинде къысмындаки пунктына маддесиндеки пунктына маддесинде къысмын даки пунктына маддесине сенеси майыс къабул олунгъан къырым джумхуриети девлет шурасы къырым джумхуриетининъ парламенти акъкъында санлы къырым джумхуриети къанунынынъ\n",
            "=== постановление государственного совета республики крым о сог ласовании назна чения кабанова на должность заместителя председателя совета министров республики крым в соответствии с пунктом части статьи пунктом статьи пунктом части статьи статьей конституции республики крым пунктом части статьи пунктом части статьи закона республики крым от мая года о государствен ном совете\n",
            ">>>  постановление государственного совета республики крым о сог ласовании назна чения на должность заместителя председателя совета министров республики крым янаки в соответствии с пунктом части статьи пунктом части статьи пунктом части статьи пунктом части статьи пунктом части статьи пунктом части статьи закона республики крым от мая года о государствен ном совете\n",
            "********************\n",
            "<<< тасвирий санат\n",
            "=== изобразительное искусство\n",
            ">>>  внесенный управление\n",
            "********************\n",
            "<<< пунктында биринджи сатырбашында сайысы сайысына денъиштирильсин сайысы сайысына денъиштирильсин учюнджи сатырбашында сайысы сайысына денъиштирильсин алтынджы сатырбашында сайысы сайысына денъиштирильсин единджи сатырбашында сайысы сайысына денъиштирильсин сайысы сайысына денъиштирильсин секизинджи сатырбашында сайысы сайысына денъиштирильсин докъузынджы сатырбашында сайысы сайысына денъиштирильсин\n",
            "=== в пункте в абзаце первом число заменить на число заменить на в абзаце третьем число заменить на в абзаце шестом число заменить на в абзаце седьмом число заменить на число заменить на в абзаце восьмом число заменить на в абзаце девятом число заменить на\n",
            ">>>  в пункте в абзаце первом число заменить на число заменить на в абзаце пятом число заменить на в абзаце девятом число заменить на в абзаце пятом число заменить на в абзаце пятом число заменить на в абзаце пятом число заменить на в абзаце девятом число заменить на\n",
            "********************\n",
            "<<< якъынлыкъ\n",
            "=== близость\n",
            ">>>  близкий\n",
            "********************\n",
            "<<< столгъа буюрынъыз\n",
            "=== пожалуйте к столу\n",
            ">>>  не знаетесь\n",
            "********************\n",
            "<<< къырым джумхуриети везирлер шурасынынъ реис муавини вазифесине игорь нико лаевич михайличенко тайинленмеси уйгъунлаштырылсын\n",
            "=== согласовать назначение михайличенко игоря николаевича на должность замести теля председателя совета министров республики крым\n",
            ">>>  согласовать освобождение жуковой юлии михайловны на должности заместителя председателя совета министров республики крым\n",
            "********************\n",
            "<<< бугунь ава аджайиптир\n",
            "=== сегодня чудесная погода\n",
            ">>>  сегодня пасмурно\n",
            "********************\n",
            "<<< мен пек оксюрем\n",
            "=== у меня сильный кашель\n",
            ">>>  я очень красивая\n",
            "********************\n",
            "<<< нарат\n",
            "=== ель\n",
            ">>>  сосна\n",
            "********************\n",
            "<<< мезкюр къарарнен тасдикълангъан сенелерине къырым мухтар джумхуриетинде кичик саипкярлыкъкъа къол тутув ве инкишафы програмнынъ илявесинде болюгинде пунктында\n",
            "=== в приложении к программе поддержки и развития малого предпринимательства в автономной республике крым на годы утвержденной данным постановлением в разделе в пункте в графе подпункта\n",
            ">>>  внести в программу развития малого предпринимательства в автономной республике крым на годы утвержденную постановлением верховной рады автономной республики крым от февраля года сборник актов автономной республики крым \n",
            "********************\n",
            "<<< мен къаршым\n",
            "=== я возражаю\n",
            ">>>  я против\n",
            "********************\n",
            "<<< дерджде луганск виляет девлет мемуриетининъ ресмий фаджебоок саифесинден ве луганск арбий мемуриетининъ реберининъ ресмий телеграм каналындан алынгъан малюмат къулланылды виляетининъ топрагъында арбий арекетлер девам эткени себебинден виляетининъ вазиетини огренмек шу анда имкянсыз къалыр\n",
            "=== в публикации использованы данные с официальной луганской ога и официального руководителя луганской областной военной администрации поскольку боевые действия на территории области продолжаются привести полную и точную статистику по ситуации на неподконтрольных территориях пока невозможно\n",
            ">>>  в публикации использована информация с официальной запорожской областной государственной администрации поскольку боевые действия на территории области продолжаются привести полную и точную статистику по ситуации на неподконтрольных территориях пока невозможно\n",
            "********************\n",
            "<<< сенеси гемининъ мулькиет акъкъына даир шеадетна меси иншаат сенеси иншаат ери килия ш\n",
            "=== год постройки место постройки килия\n",
            ">>>  год постройки место постройки килия\n",
            "********************\n",
            "<<< реинтеграция назирлиги девлет ичинде алынгъан балаларны къыш урбасынен темин этюв акъкъында эсабат бере\n",
            "=== минреинтеграции отчитывается об обеспечении детей переселенцев комплектами теплой одежды\n",
            ">>>  минреинтеграции обеспечения одеждает детей осуществляется юридических лиц внутренних переселенцев\n",
            "********************\n",
            "<<< банкта пара эспкъа алмакъ ичюн реквизитлер малюматнамесини алынъыз\n",
            "=== возьмите в банке справку с реквизитами для зачисления средств\n",
            ">>>  предлагается средства получили работу представлению предприятию индивидуальных предприни\n",
            "********************\n",
            "<<< тур ве экскурсияларнынъ тарифи олгъан бир рисаленъиз бармы\n",
            "=== у вас есть брошюра с описанием туров и экскурсий\n",
            ">>>  идите постановление где экскурсии и через экскурсию\n",
            "********************\n",
            "<<< ашагъыдаки федераль къанун лейхаларына дестек берильмесин русие федерациясынынъ айры къанунджылыкъ весикъаларына денъиш мелер кирсетильмеси акъкъында сенеси июль къабул олунгъан санлы федераль къануны кучюни гъайып эткен оларакъ танылмасы акъкъында\n",
            "=== не поддерживать следующие проекты федеральных законов о признании утратившим силу федерального закона от июля года о внесении изменений в отдельные законодательные акты российской федерации\n",
            ">>>  поддержать следующие проекты федеральных законов о внесении изменений в федеральный закон об об основах охраны здоровья граждан в российской федерации в федеральный закон о развитии малого и среднего предпринимательства\n",
            "********************\n",
            "<<< къырым джумхуриети девлет шурасы къырым джумхуриетининъ парламенти акъкъында къырым джумхуриети къанунынынъ маддесине денъишмелер акъкъында къырым джумхуриетининъ къанун лейхасыны экинджи окъувгъа азырламакъ ве къырым джумхуриети девлет шурасынынъ бакъымына кирсетмек къырым джумхури ети девлет шурасынынъ девлет къуруджылыкъ ве ерли мустакъиль идареджилик меселелери боюн джа комитетине авале этильсин\n",
            "=== поручить комитету государственного совета республики крым по вопросам государ ственного строительства и местного самоуправления подготовить проект закона республи ки крым о внесении изменения в статью закона республики крым о государственном сове те республики крым парламенте республики крым ко второму чтению и внести его на рассмотрение государственного совета республики крым текст\n",
            ">>>  поручить комитету государственного совета республики крым по государственно му строительству и местному самоуправлению подготовить проект закона республики крым о внесении изменения в статью закона республики крым о государственном совете республики крым парламенте республики крым ко второму чтению и внести его на рассмотрение государственного совета республики крым\n",
            "********************\n",
            "<<< къырымда ишгъаль акимиет къырымтатарларгъа ве вакътынджа ишгъаль этильген ярымаданынъ башкъа тамыр халкъларына къаршы геноцидни девам эте бу кез исе ишгъальджильнинъ тарафында дженклешмелери ичюн къанунсыз кутьлевий мобилизация шеклинде\n",
            "=== оккупационные власти в крыму продолжают геноцид против крымских татар и других коренных народов временно оккупированного полуострова на этот раз в форме незаконной массовой мобилизации для участия в войне на стороне оккупантов\n",
            ">>>  оккупанты пытаются принудительную депортацию международной организации власти и оккупанты преступления в крыму оккупанты есть принудительную мобилизацию к народу рф против украинских военных войны\n",
            "********************\n",
            "<<< бу иляджны насыл ичмели\n",
            "=== как принимать это лекарство\n",
            ">>>  как это лекарство пить\n",
            "********************\n",
            "<<< пунктында сайысы сайысына денъиштирильсин\n",
            "=== в пункте число заменить на\n",
            ">>>  в пункте число заменить на\n",
            "********************\n",
            "<<< диний дуйгъулар\n",
            "=== чувства религиозные\n",
            ">>>  проствая моя\n",
            "********************\n"
          ]
        }
      ],
      "source": [
        "for example in list(test_data)[600:660]:\n",
        "  print(\"<<<\", example['crh'])\n",
        "  print(\"===\", example['rus'])\n",
        "  print(\">>>\", translate(transformer, example['crh']))\n",
        "  print(\"*\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(transformer, 'внесенный главой республики крым аксёновым государственный совет республики крым постановляет')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcJDWpIed4hg",
        "outputId": "7d86b785-1e22-46be-862c-7dc60060a58e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' капитальный ремонт советский пер архюродная'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKmt-FJanRW7",
        "outputId": "15b10959-82ad-4b45-fdee-883b85da161d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [06:56<00:00,  2.40it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate import bleu_score\n",
        "import tqdm\n",
        "\n",
        "target = []\n",
        "predictions = []\n",
        "\n",
        "for example in tqdm.tqdm(list(test_data)[:1000]):\n",
        "  pred = translate(transformer, example['crh']).strip()\n",
        "  pred = token_transform[TGT_LANGUAGE].encode(pred).tokens\n",
        "  trg = example['rus'].rstrip(\"\\n\").strip()\n",
        "  trg = token_transform[TGT_LANGUAGE].encode(trg).tokens\n",
        "  \n",
        "  target.append(trg)\n",
        "  predictions.append([pred])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_vjL2IQVJq8",
        "outputId": "11601011-e193-41c8-f64a-f5a015c13cac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5183140124796218"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "bleu_score.corpus_bleu(predictions, target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(transformer.state_dict(), \"crh_to_rus.weights\")"
      ],
      "metadata": {
        "id": "doUZlPIltS3c"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sxVQ8FEVJq9"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Attention is all you need paper.\n",
        "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}